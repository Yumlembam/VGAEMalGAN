{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from glob import glob\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from tqdm import tqdm\n",
    "from scipy import sparse\n",
    "from torch_geometric.nn import GraphConv, TopKPooling, GatedGraphConv, SAGEConv, SGConv\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "np.random.seed(42)\n",
    "from sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score,accuracy_score\n",
    "from torch_geometric.nn import VGAE\n",
    "from torch import nn\n",
    "import torch_geometric\n",
    "from torch_geometric.data import DataLoader\n",
    "from sklearn import metrics\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(mat):\n",
    "#     mat = sparse.load_npz(path)\n",
    "#     print(mat.shape)\n",
    "    mat_coo = mat.tocoo()\n",
    "    mat_csr = mat_coo.tocsr()\n",
    "    G_new = nx.Graph(mat_csr)\n",
    "    return G_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        total_feature = pickle.load(f)\n",
    "        return total_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-registration",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_feature= load_feature(r'D:\\android_gan_mal2020\\code_total_feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional,SimpleRNN,GRU,Reshape,TimeDistributed,concatenate,BatchNormalization\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Activation, Flatten, UpSampling1D,MaxPooling1D\n",
    "# from tensorflow.keras.layers.convolutional import MaxPooling1D\n",
    "# from tensorflow.keras.layers.advanced_activations import LeakyReLU\n",
    "# from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import model_from_json\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.backend.tensorflow_backend import set_session\n",
    "# from tensorflow.keras.backend.tensorflow_backend import clear_session\n",
    "# from tensorflow.keras.backend.tensorflow_backend import get_session  \n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-belfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open(r\"cnn98-12-nodiag.json\", 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "black_box_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "black_box_model.load_weights(r\"cnn98-12-nodiag.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSage(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GraphSage, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = SAGEConv(5, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, 2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv5(x, edge_index)\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        emb=x\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.lin1(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.lin3(x)   \n",
    "        x = self.lin(x)\n",
    "        return emb,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "black_box_emb_model = GraphSage(hidden_channels=32)\n",
    "optimizer_bb = torch.optim.Adam(black_box_emb_model.parameters(), lr=0.01)\n",
    "criterion_bb = torch.nn.CrossEntropyLoss()\n",
    "black_box_emb_model.load_state_dict(torch.load(r\"GraphSagemal2020full_94acc.pt\"))\n",
    "black_box_emb_model = black_box_emb_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalGCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(VariationalGCNEncoder, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, out_channels)\n",
    "        self.conv5 = SAGEConv(out_channels, out_channels)\n",
    "        self.conv_mu = SAGEConv(out_channels, out_channels)\n",
    "        self.conv_logstd = SAGEConv(out_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv5(x, edge_index).relu()\n",
    "        return self.conv_mu(x, edge_index), self.conv_logstd(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae_generator = VGAE(VariationalGCNEncoder(5,32))\n",
    "optimizer_vae_generator = torch.optim.Adam(vae_generator.parameters(), lr=0.01)\n",
    "# vae_generator.load_state_dict(torch.load(r\"vgae_mal_final_no_diag.pt\"))\n",
    "vae_generator = vae_generator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSageDis(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GraphSageDis, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = SAGEConv(5, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, 2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv5(x, edge_index)\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        emb=x\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.lin1(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.lin3(x)   \n",
    "        x = self.lin(x)\n",
    "        return emb,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator_model = GraphSageDis(hidden_channels=32)\n",
    "optimizer_discriminator = torch.optim.Adam(discriminator_model.parameters(), lr=0.01)\n",
    "criterion_discriminator = torch.nn.CrossEntropyLoss()\n",
    "discriminator_model = discriminator_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnsemble(torch.nn.Module):\n",
    "    def __init__(self, vae_generator, discriminator):\n",
    "        super(MyEnsemble, self).__init__()\n",
    "        self.vae_generator = vae_generator\n",
    "        self.discriminator = discriminator\n",
    "        \n",
    "    def forward(self, x, edge_index,batch,full,permission):\n",
    "        z = self.vae_generator.encode(x,edge_index)\n",
    "#         print(z)\n",
    "        adj=torch.exp(-torch.cdist(z,z, p=2.0))\n",
    "        adj = (adj>0.98).float()*1\n",
    "#         print(adj)\n",
    "        #to remove self loop\n",
    "        generated_edge_index_sparse=torch_geometric.utils.dense_to_sparse(adj)[0]    \n",
    "         # self loop remove\n",
    "        generated_edge_index_sparse=torch_geometric.utils.remove_self_loops(generated_edge_index_sparse)[0]\n",
    "        try:\n",
    "        # convert back to adjacency matrix to perform and and or operation\n",
    "            generated_edge_index_dense=torch_geometric.utils.to_dense_adj(generated_edge_index_sparse,max_num_nodes=1000)[0]\n",
    "        except:\n",
    "            generated_edge_index_dense=torch_geometric.utils.to_dense_adj(edge_index,max_num_nodes=1000)[0]\n",
    "        #perform and operation with full graph\n",
    "        generated_edge_index_valid_links=torch.logical_and(full,generated_edge_index_dense)*1\n",
    "        #perform or operation with the original edge_index to preserver the malware characteristic\n",
    "        edge_index_dense=torch_geometric.utils.to_dense_adj(edge_index,max_num_nodes=1000)[0]\n",
    "        generated_edge_index_final=torch.logical_or(generated_edge_index_valid_links,edge_index_dense)*1\n",
    "        #cover it into sparse to pass into discriminator\n",
    "        generated_edge_index_final=torch_geometric.utils.dense_to_sparse(generated_edge_index_final)[0]\n",
    "        emb,output = self.discriminator(x,generated_edge_index_final,batch)\n",
    "        return output,z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator=MyEnsemble(vae_generator, discriminator_model)\n",
    "generator=generator.to(device)\n",
    "print(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall_graph_path=r'code_block_id_tr_reduced_tr.npz'\n",
    "# sparse_overall_graph = sparse.load_npz(overall_graph_path)\n",
    "# overall_graph_edge_index_coo=sparse_overall_graph.tocoo()\n",
    "# overall_graph_edge_index = torch.tensor([overall_graph_edge_index_coo.row.tolist(),overall_graph_edge_index_coo.col.tolist()], dtype=torch.long) \n",
    "# pos_overall_graph_edge_index = overall_graph_edge_index.to(device)\n",
    "# pos_overall_graph_edge_index =torch_geometric.utils.remove_self_loops(pos_overall_graph_edge_index)[0]\n",
    "# pos_overall_adj=torch_geometric.utils.to_dense_adj(pos_overall_graph_edge_index)[0]\n",
    "# print(pos_overall_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_graph=load_feature(\"overall_edge_list\")\n",
    "overall_graph_edge_index = torch.tensor([overall_graph[0],overall_graph[1]], dtype=torch.long) \n",
    "pos_overall_graph_edge_index = overall_graph_edge_index.to(device)\n",
    "pos_overall_graph_edge_index = torch_geometric.utils.remove_self_loops(pos_overall_graph_edge_index)[0]\n",
    "pos_overall_adj=torch_geometric.utils.to_dense_adj(pos_overall_graph_edge_index)[0]\n",
    "print(pos_overall_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_df=pd.read_csv(r'permission_reduced_mal2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_df.iloc[:,1:6852]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading dataset malware\n",
    "class YooChooseDatasetBen(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(YooChooseDatasetBen, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [r'code_block.dataset']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "        \n",
    "    def process(self):\n",
    "        data_list = []\n",
    "        edge_index_path=[r\"class0\"]\n",
    "        current=0\n",
    "        for i in edge_index_path: \n",
    "            edge_indexes = glob(f'{i}/*.npz')\n",
    "            print(len(edge_indexes))\n",
    "            for edge_index_path in edge_indexes:\n",
    "                current=current+1\n",
    "                print(current)\n",
    "                y=[0]\n",
    "                y = torch.tensor(y)\n",
    "                try:\n",
    "                    sparse_arr = sparse.load_npz(edge_index_path)\n",
    "                    sparse_arr.setdiag(0)\n",
    "                    sparse_arr.eliminate_zeros()\n",
    "                    edge_index_processed=sparse_arr\n",
    "                    edge_index = torch.tensor([edge_index_processed.row.tolist(),edge_index_processed.col.tolist()], dtype=torch.long)\n",
    "                    x = torch.tensor(total_feature,dtype=torch.float)\n",
    "                    app_name=os.path.basename(edge_index_path).replace('.npz','')\n",
    "                    permission_data=perm_df.loc[perm_df['App']==app_name].iloc[:,1:6852].values.tolist()[0]\n",
    "                    data = Data(x=x, edge_index=edge_index, y=y,permission=permission_data)\n",
    "                    data_list.append(data)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(edge_index_path)\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset_benign= YooChooseDatasetBen(root=r'bening_gan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "dataset_benign = custom_dataset_benign.shuffle()\n",
    "train_dataset_benign = dataset_benign[:2586]\n",
    "test_dataset_benign = dataset_benign[2586:]\n",
    "print(test_dataset_benign)\n",
    "print(f'Number of training graphs: {len(train_dataset_benign)}')\n",
    "print(f'Number of test graphs: {len(test_dataset_benign)}')\n",
    "print(dataset_benign.num_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-organ",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_train(loader):\n",
    "    discriminator_model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    benign_count=0\n",
    "    malware_count=0\n",
    "    for data in loader:  # Iterate in batches over the training dataset.\n",
    "         data.x = data.x.to(device)\n",
    "         data.edge_index= data.edge_index.to(device)\n",
    "         data.batch=data.batch.to(device)\n",
    "         data.y = data.y.to(device)\n",
    "         emb,out = discriminator_model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "#          print(out)\n",
    "         pred = out.argmax(dim=1)\n",
    "#          print(pred.dtype)\n",
    "#          print(data.y.dtype)\n",
    "         output=pred.cpu().numpy()\n",
    "         for i in output:\n",
    "            if i ==0:\n",
    "                benign_count+=1\n",
    "            else:\n",
    "                malware_count+=1\n",
    "         \n",
    "         correct += int((pred == data.y).sum())\n",
    "#          print(pred[0].dtype)\n",
    "#          print(data.y[0].dtype)\n",
    "#          print(\"prediction\",pred)\n",
    "#          print(\"data y\",data.y)\n",
    "         loss = criterion_discriminator(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer_discriminator.step()  # Update parameters based on gradients.\n",
    "         optimizer_discriminator.zero_grad() # Clear gradients.\n",
    "         running_loss += loss.item() * data.num_graphs\n",
    "    running_loss /= len(loader.dataset)\n",
    "    print(benign_count)\n",
    "    print(malware_count)\n",
    "    return correct / len(loader.dataset),running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-regard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discriminator_prediction(data):\n",
    "    discriminator_model.eval()\n",
    "    data=data.to(device)\n",
    "    out= discriminator_model(data.x, data.edge_index, data.batch)  \n",
    "    pred = out.argmax(dim=1)\n",
    "    pred_numpy = pred.cpu().numpy()\n",
    "    return pred_numpy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mydecoder(edge_index,z,pos_overall_adj):\n",
    "    adj=torch.exp(-torch.cdist(z,z, p=2.0))\n",
    "    adj = (adj>0.98).float()*1\n",
    "#     adj = torch.matmul(z, z.t())\n",
    "#     adj=torch.sigmoid(adj)\n",
    "    #to remove self loop\n",
    "    generated_edge_index_sparse=torch_geometric.utils.dense_to_sparse(adj)[0] \n",
    "    \n",
    "     # self loop remove\n",
    "    generated_edge_index_sparse=torch_geometric.utils.remove_self_loops(generated_edge_index_sparse)[0]\n",
    "    # convert back to adjacency matrix to perform and and or operation\n",
    "    try:\n",
    "        generated_edge_index_dense=torch_geometric.utils.to_dense_adj(generated_edge_index_sparse,max_num_nodes=1000)[0]\n",
    "        exception=False\n",
    "    except:\n",
    "        generated_edge_index_dense=torch_geometric.utils.to_dense_adj(edge_index,max_num_nodes=1000)[0]\n",
    "        exception=True\n",
    "#     print(pos_overall_adj)\n",
    "    #perform and operation with full graph\n",
    "    generated_edge_index_valid_links=torch.logical_and(pos_overall_adj,generated_edge_index_dense)*1\n",
    "    #print(generated_edge_index_valid_links)\n",
    "    #perform or operation with the original edge_index to preserver the malware characteristic\n",
    "    edge_index_dense=torch_geometric.utils.to_dense_adj(edge_index,max_num_nodes=1000)[0]\n",
    "    generated_edge_index_final=torch.logical_or(generated_edge_index_valid_links,edge_index_dense)*1\n",
    "    #cover it into sparse to pass into discriminator\n",
    "    generated_edge_index_final=torch_geometric.utils.dense_to_sparse(generated_edge_index_final)[0]\n",
    "    return generated_edge_index_final,exception\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frequent-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_black_box_p_emb_prediction(permission_data,embedding):\n",
    "    pred_soft=black_box_model.predict([np.array(permission_data).reshape(1,6851),embedding.detach().cpu().numpy()])\n",
    "    if pred_soft[0]<0.5:\n",
    "        pred=0\n",
    "    else:\n",
    "        pred=1\n",
    "    return torch.tensor([pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_list=[]\n",
    "for data in train_dataset_benign:\n",
    "    data.y= torch.tensor([0])\n",
    "    benign_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-edgar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positive_rate_calculator(prediction_list,true_list):\n",
    "    confusion=confusion_matrix(true_list,prediction_list)\n",
    "    print(confusion)\n",
    "    TP=confusion[1][1]\n",
    "    FN=confusion[1][0]\n",
    "    true_positive_rate=TP/(TP+FN)\n",
    "    return true_positive_rate\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-request",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(data):\n",
    "    discriminator_model.eval()\n",
    "    data=data.to(device)\n",
    "    emb,out = discriminator_model(data.x, data.edge_index, data.batch)  \n",
    "    pred = out.argmax(dim=1)\n",
    "    return pred,emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_bb(data):\n",
    "    black_box_emb_model.eval()\n",
    "    data=data.to(device)\n",
    "    emb,out = black_box_emb_model(data.x, data.edge_index, data.batch)  \n",
    "    pred = out.argmax(dim=1)\n",
    "    return pred,emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "malware_network_perm=[]\n",
    "edge_index_path=[r\"class1\"]\n",
    "current=0\n",
    "exception_count=0\n",
    "generated_malware_list=[]\n",
    "generated_prediction=[]\n",
    "for i in edge_index_path: \n",
    "    edge_indexes = glob(f'{i}/*.npz')\n",
    "    for edge_index_path in edge_indexes:\n",
    "        current=current+1\n",
    "        #get app name\n",
    "        app_name=os.path.basename(edge_index_path).replace('.npz','')\n",
    "        #get permission\n",
    "        permission_data=perm_df.loc[perm_df['App']==app_name].iloc[:,1:6852].values.tolist()[0]\n",
    "        #load network from data from path\n",
    "        sparse_arr = sparse.load_npz(edge_index_path)\n",
    "        #transform intoo coo\n",
    "        sparse_arr.setdiag(0)\n",
    "        sparse_arr.eliminate_zeros()\n",
    "        edge_index_processed=sparse_arr\n",
    "        #convert in to edge index\n",
    "        edge_index = torch.tensor([edge_index_processed.row.tolist(),edge_index_processed.col.tolist()], dtype=torch.long)\n",
    "        malware_network_perm.append((edge_index,permission_data))\n",
    "        if current==8474:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-oracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(malware_network_perm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "true_positive_calcu=[]\n",
    "#for batching will use data loader to batch and pass on to discriminator\n",
    "for epoch in range(100):\n",
    "    print(\"current iteration\",epoch)\n",
    "    generated_malware_list=[]\n",
    "    exception_count=0\n",
    "    generated_prediction=[]\n",
    "    try:\n",
    "        current=0\n",
    "        exception_count=0\n",
    "        generated_malware_list=[]\n",
    "        generated_prediction=[]\n",
    "        for edge_perm in malware_network_perm:\n",
    "            current=current+1\n",
    "            #get permission\n",
    "            permission_data=edge_perm[1]\n",
    "            edge_index = edge_perm[0]\n",
    "            #get the x feature\n",
    "            x = torch.tensor(total_feature,dtype=torch.float)\n",
    "            random_data = random.choice(benign_list)\n",
    "            edge_index_0=random_data.edge_index[0].split(int(len(random_data.edge_index[0])/2))[1]\n",
    "            edge_index_1=random_data.edge_index[1].split(int(len(random_data.edge_index[1])/2))[1]\n",
    "            benign_noise = torch.empty([2,int(len(random_data.edge_index[1])/2)],dtype=int)\n",
    "            benign_noise[0]=edge_index_0\n",
    "            benign_noise[1]=edge_index_1\n",
    "            noise_edge_index=torch.cat((edge_index,benign_noise),1)\n",
    "            malware = Data(x=x, edge_index=noise_edge_index)\n",
    "            malware=malware.to(device)\n",
    "            z = vae_generator.encode(malware.x,malware.edge_index)\n",
    "            edge_index,exception=mydecoder(malware.edge_index,z,pos_overall_adj)\n",
    "            if exception:\n",
    "                exception_count+=1\n",
    "            x = torch.tensor(total_feature,dtype=torch.float)\n",
    "            data = Data(x=x, edge_index=edge_index)\n",
    "            data.batch=torch.zeros(1000, dtype=torch.long)\n",
    "            out_graph,emb=get_embedding_bb(data)\n",
    "            out_perm_graph=get_black_box_p_emb_prediction(permission_data,emb)\n",
    "            generated_prediction.append(out_perm_graph)\n",
    "            data.edge_index.cpu()\n",
    "            data.x=data.x.cpu()\n",
    "            data.batch=data.batch.cpu()\n",
    "            data.y=out_perm_graph\n",
    "            data.batch=None\n",
    "            generated_malware_list.append(data)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    print(\"malware generation complete\")\n",
    "    print(\"graph generation\",exception_count)\n",
    "    pred_list_list=[l.tolist() for l in generated_prediction]\n",
    "    predlist = [item for sublist in pred_list_list for item in sublist]\n",
    "    # print(predlist)\n",
    "    predlist.append(0)\n",
    "    predlist.append(1)\n",
    "    true_label_check=[1]*len(predlist)\n",
    "    print(\"true positive rate after generation\",true_positive_rate_calculator(predlist,true_label_check))\n",
    "    tpr=true_positive_rate_calculator(predlist,true_label_check)\n",
    "    if tpr<=0.60:\n",
    "        print(\"saved here 0.60\")\n",
    "        torch.save(vae_generator.state_dict(), r\"newvgaecnnbb_final_\"+str(tpr)+\".pt\")\n",
    "    if tpr<=0.50:\n",
    "        print(\"saved here 0.50\")\n",
    "        torch.save(vae_generator.state_dict(), r\"newvgaecnnbb_final_\"+str(tpr)+\".pt\")\n",
    "    if tpr<=0.40:\n",
    "        print(\"saved here 40\")\n",
    "        torch.save(vae_generator.state_dict(), r\"newvgaecnnbb_final_\"+str(tpr)+\".pt\")\n",
    "    #getting true positive rate after graph generation\n",
    "    #combine_list=generated_malware_list+benign_list\n",
    "    benign_train_loader = DataLoader(benign_list, batch_size=64, shuffle=True)\n",
    "    malware_train_loader= DataLoader(generated_malware_list, batch_size=64, shuffle=True)\n",
    "    for inner_epoch in range(3):\n",
    "        train_acc_benign,train_loss_benign = discriminator_train(benign_train_loader)\n",
    "        print(\"training benign discriminator\")\n",
    "        print(f'Epoch: {inner_epoch:03d}, discriminator Train Acc: {train_acc_benign:.4f},discriminator Train loss:{train_loss_benign:.4f}')\n",
    "    for inner_epoch in range(10):\n",
    "        train_acc_malware,train_loss_malware = discriminator_train(malware_train_loader)\n",
    "        print(\"training malware discriminator\")\n",
    "        print(f'Epoch: {inner_epoch:03d}, discriminator Train Acc: {train_acc_malware:.4f},discriminator Train loss:{train_loss_malware:.4f}')\n",
    "    #---------------------------------------------------------------------------\n",
    "    # train generator\n",
    "    for generator_epoch in range(3):\n",
    "        running_loss = 0.0\n",
    "        running_loss_dis=0.0\n",
    "        current=0\n",
    "        exception_count=0\n",
    "        generated_malware_list=[]\n",
    "        generated_prediction=[]\n",
    "        for edge_perm in malware_network_perm:\n",
    "            current=current+1\n",
    "            #get permission\n",
    "            permission_data=edge_perm[1]\n",
    "            #load network from data from path\n",
    "            #convert in to edge index\n",
    "            edge_index = edge_perm[0]\n",
    "            #get the x feature\n",
    "            x = torch.tensor(total_feature,dtype=torch.float)\n",
    "            random_data = random.choice(benign_list)\n",
    "            edge_index_0=random_data.edge_index[0].split(int(len(random_data.edge_index[0])/2))[1]\n",
    "            edge_index_1=random_data.edge_index[1].split(int(len(random_data.edge_index[1])/2))[1]\n",
    "            benign_noise = torch.empty([2,int(len(random_data.edge_index[1])/2)],dtype=int)\n",
    "            benign_noise[0]=edge_index_0\n",
    "            benign_noise[1]=edge_index_1\n",
    "            noise_edge_index=torch.cat((edge_index,benign_noise),1)\n",
    "            data = Data(x=x, edge_index=noise_edge_index)\n",
    "            data.batch=torch.zeros(1000, dtype=torch.long)\n",
    "            data.y=torch.tensor([0])\n",
    "            data=data.to(device)\n",
    "            out,z=generator(data.x,data.edge_index,data.batch,pos_overall_adj,permission_data)\n",
    "            out=out.to(device)\n",
    "            loss = vae_generator.recon_loss(z,data.edge_index)\n",
    "            loss_discriminator = criterion_discriminator(out, data.y)\n",
    "#             loss=loss_discriminator\n",
    "            loss = loss+(1 / data.num_nodes) * vae_generator.kl_loss()+loss_discriminator\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer_vae_generator.step()  # Update parameters based using gradients.\n",
    "            optimizer_vae_generator.zero_grad() # Clear gradients.\n",
    "            running_loss += loss \n",
    "            running_loss_dis += loss_discriminator \n",
    "            data.batch=None\n",
    "            data.edge_index=data.edge_index.cpu()\n",
    "            data.x=data.x.cpu()\n",
    "            data.y=data.y.cpu()\n",
    "        print(\"final_loss\")\n",
    "        running_loss /= len(edge_indexes)\n",
    "        running_loss_dis /= len(edge_indexes)\n",
    "        vgae_loss= running_loss - running_loss_dis\n",
    "        print(\"loss of vgae\",vgae_loss)\n",
    "        print(\"loss of generator\",running_loss)\n",
    "        print('loss of discriminator',running_loss_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-beverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.binary_cross_entropy_with_logits(torch.tensor([0.0]),torch.tensor([0.1]),reduce=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-college",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count=0\n",
    "true_positive_calcu=[]\n",
    "#for batching will use data loader to batch and pass on to discriminator\n",
    "for epoch in range(100):\n",
    "    print(\"current iteration\",epoch)\n",
    "    generated_malware_list=[]\n",
    "    exception_count=0\n",
    "    for malware in train_dataset_malware:\n",
    "        \n",
    "        try:\n",
    "            random_data = random.choice(benign_list)\n",
    "            edge_index_0=random_data.edge_index[0].split(int(len(random_data.edge_index[0])/2))[0]\n",
    "            edge_index_1=random_data.edge_index[1].split(int(len(random_data.edge_index[1])/2))[0]\n",
    "            benign_noise = torch.empty([2,int(len(random_data.edge_index[1])/2)],dtype=int)\n",
    "            benign_noise[0]=edge_index_0\n",
    "            benign_noise[1]=edge_index_1\n",
    "            noise_edge_index=torch.cat((malware.edge_index,benign_noise),1)\n",
    "            malware.edge_index=noise_edge_index\n",
    "            count=count+1\n",
    "            malware=malware.to(device)\n",
    "            z = vae_generator.encode(malware.x,malware.edge_index)\n",
    "            edge_index,exception=mydecoder(malware.edge_index,z,pos_overall_adj)\n",
    "            if exception:\n",
    "                exception_count+=1\n",
    "            x = torch.tensor(total_feature,dtype=torch.float)\n",
    "            data = Data(x=x, edge_index=edge_index)\n",
    "            data.batch=torch.zeros(7017, dtype=torch.long)\n",
    "            #get the black_box prediction\n",
    "            out = get_black_box_prediction(data).cpu()\n",
    "            data.edge_index=data.edge_index.cpu()\n",
    "            data.edge_index.cpu()\n",
    "            data.x=data.x.cpu()\n",
    "            data.batch=data.batch.cpu()\n",
    "            data.y=out.cpu()\n",
    "            out = out.cpu().numpy()\n",
    "            data.batch=None\n",
    "            generated_malware_list.append(data)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    print(\"malware generation complete\")\n",
    "    print(\"graph generation\",exception_count)\n",
    "    #combine_list=generated_malware_list+benign_list\n",
    "    benign_train_loader = DataLoader(benign_list, batch_size=64, shuffle=True)\n",
    "    malware_train_loader= DataLoader(generated_malware_list, batch_size=64, shuffle=True)\n",
    "    train_acc_benign,train_loss_benign = discriminator_train(benign_train_loader)\n",
    "    print(\"training benign discriminator\")\n",
    "    print(f'Epoch: {epoch:03d}, discriminator Train Acc: {train_acc_benign:.4f},discriminator Train loss:{train_loss_benign:.4f}')\n",
    "    for inner_epoch in range(10):\n",
    "        train_acc_malware,train_loss_malware = discriminator_train(malware_train_loader)\n",
    "        print(\"training malware discriminator\")\n",
    "        print(f'Epoch: {inner_epoch:03d}, discriminator Train Acc: {train_acc_malware:.4f},discriminator Train loss:{train_loss_malware:.4f}')\n",
    "\n",
    "    #---------------------------------------------------------------------------\n",
    "    # train generator\n",
    "    for generator_epoch in range(3):\n",
    "        running_loss = 0.0\n",
    "        running_loss_dis=0.0\n",
    "        for data in train_dataset_malware:\n",
    "            data.y=torch.tensor([0])\n",
    "            data.batch=torch.zeros(7017, dtype=torch.long)\n",
    "            random_data = random.choice(benign_list)\n",
    "            edge_index_0=random_data.edge_index[0].split(int(len(random_data.edge_index[0])/2))[0]\n",
    "            edge_index_1=random_data.edge_index[1].split(int(len(random_data.edge_index[1])/2))[0]\n",
    "            benign_noise = torch.empty([2,int(len(random_data.edge_index[1])/2)],dtype=int)\n",
    "            benign_noise[0]=edge_index_0\n",
    "            benign_noise[1]=edge_index_1\n",
    "            noise_edge_index=torch.cat((data.edge_index,benign_noise),1)\n",
    "            data.edge_index=noise_edge_index\n",
    "            data=data.to(device)\n",
    "\n",
    "            out,z=generator(data.x,data.edge_index,data.batch,pos_overall_adj)\n",
    "            loss = vae_generator.recon_loss(z,data.edge_index)\n",
    "            loss_discriminator = criterion_discriminator(out, data.y)\n",
    "    #        loss=loss_discriminator\n",
    "    #         print(loss_discriminator)\n",
    "            loss = loss+(1 / data.num_nodes) * vae_generator.kl_loss()+loss_discriminator\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer_vae_generator.step()  # Update parameters based on gradients.\n",
    "            optimizer_vae_generator.zero_grad() # Clear gradients.\n",
    "            running_loss += loss \n",
    "            running_loss_dis += loss_discriminator \n",
    "            data.batch=None\n",
    "            data.edge_index=data.edge_index.cpu()\n",
    "            data.x=data.x.cpu()\n",
    "            data.y=data.y.cpu()\n",
    "            pred = out.argmax(dim=1)\n",
    "        print(\"final_loss\")\n",
    "        running_loss /= len(train_dataset_malware)\n",
    "        running_loss_dis /= len(train_dataset_malware)\n",
    "        print(\"loss of generator\",running_loss)\n",
    "        print('loss of discriminator',running_loss_dis)\n",
    "    #getting tpr after training generator\n",
    "    generated_malware_list=[]\n",
    "    prediction=[]\n",
    "    exception_count=0\n",
    "    for malware in train_dataset_malware:\n",
    "        random_data = random.choice(benign_list)\n",
    "        edge_index_0=random_data.edge_index[0].split(int(len(random_data.edge_index[0])/2))[0]\n",
    "        edge_index_1=random_data.edge_index[1].split(int(len(random_data.edge_index[1])/2))[0]\n",
    "        benign_noise = torch.empty([2,int(len(random_data.edge_index[1])/2)],dtype=int)\n",
    "        benign_noise[0]=edge_index_0\n",
    "        benign_noise[1]=edge_index_1\n",
    "        noise_edge_index=torch.cat((malware.edge_index,benign_noise),1)\n",
    "        malware.edge_index=noise_edge_index\n",
    "        malware=malware.to(device)\n",
    "        z = vae_generator.encode(malware.x,malware.edge_index)\n",
    "        edge_index,exception=mydecoder(malware.edge_index,z,pos_overall_adj)\n",
    "        if exception:\n",
    "            exception_count+=1\n",
    "        x = torch.tensor(total_feature,dtype=torch.float)\n",
    "        data = Data(x=x, edge_index=edge_index)\n",
    "        data.batch=torch.zeros(7017, dtype=torch.long)\n",
    "        #get the black_box prediction\n",
    "        out = get_black_box_prediction(data).cpu()\n",
    "        data.edge_index=data.edge_index.cpu()\n",
    "        data.x=data.x.cpu()\n",
    "        data.batch=data.batch.cpu()\n",
    "        data.y=out.cpu()\n",
    "        out = out.cpu().numpy()\n",
    "        prediction.append(out)\n",
    "        data.batch=None\n",
    "        generated_malware_list.append(data)\n",
    "    pred_list_list=[l.tolist() for l in prediction]\n",
    "    predlist = [item for sublist in pred_list_list for item in sublist]\n",
    "    predlist.append(0)\n",
    "    predlist.append(1)\n",
    "    # print(predlist)\n",
    "    true_label_check=[1]*len(predlist)\n",
    "    print(\"after generator graph generation exception\",exception_count)\n",
    "    tpr=true_positive_rate_calculator(predlist,true_label_check)\n",
    "    print(\"true positive rate after training generator\",tpr)\n",
    "    true_positive_calcu.append(true_positive_rate_calculator(predlist,true_label_check))\n",
    "    if tpr<=0.65:\n",
    "        print(\"saved here 0.6\")\n",
    "        torch.save(vae_generator.state_dict(), r\"D:\\android_gan\\vgae_final_\"+str(tpr)+\".pt\")\n",
    "    if tpr<=0.5:\n",
    "        print(\"saved here 0.5\")\n",
    "        torch.save(vae_generator.state_dict(), r\"D:\\android_gan\\vgae_final_\"+str(tpr)+\".pt\")\n",
    "    if tpr<=0.45:\n",
    "        print(\"saved here 0.45\")\n",
    "        torch.save(vae_generator.state_dict(), r\"D:\\android_gan\\vgae_final_\"+str(tpr)+\".pt\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
