{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from glob import glob\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from tqdm import tqdm\n",
    "from scipy import sparse\n",
    "from torch_geometric.nn import GraphConv, TopKPooling, GatedGraphConv, SAGEConv, SGConv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(path):\n",
    "    mat = sparse.load_npz(path)\n",
    "    mat_coo = mat.tocoo()\n",
    "    mat_csr = mat_coo.tocsr()\n",
    "    G_new = nx.Graph(mat_csr)\n",
    "    return G_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overall_graph_feature(G_new):\n",
    "    degree_list = []\n",
    "#     for s in G_new.degree():\n",
    "#         degree_list.append(s[1])\n",
    "#     print(\"degree calculation completed\")\n",
    "    G_new_dc=nx.degree_centrality(G_new)\n",
    "    # # print(G_new_dc)\n",
    "    print(\"degree centrality calculation completed\")\n",
    "    G_new_bc=nx.betweenness_centrality(G_new)\n",
    "    # print(G_new_bc)\n",
    "    print(\"betweeness centrality calculation completed\")\n",
    "    G_new_lc=nx.closeness_centrality(G_new)\n",
    "    # print(G_new_lc)\n",
    "    print(\"closeness centrality calculation completed\")\n",
    "    G_new_ec=nx.eigenvector_centrality(G_new)\n",
    "    # print(G_new_ec)\n",
    "    print(\"Eigen value centrality calculation completed\")\n",
    "    G_Page_rank = nx.pagerank(G_new, alpha=0.9)\n",
    "    # print(G_Page_rank)\n",
    "    print(\"Page rank calculation completed\")\n",
    "    total_feature=[]\n",
    "    for i in range(G_new.number_of_nodes()):\n",
    "      feature=[G_new_dc[i],G_new_bc[i],G_new_lc[i],G_new_ec[i],G_Page_rank[i]]\n",
    "      total_feature.append(feature)\n",
    "    return total_feature\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-favor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_total_feature(path,total_feature):\n",
    "    try:\n",
    "        total_feature_data = open(path,'wb')\n",
    "        pickle.dump(total_feature,total_feature_data)\n",
    "        total_feature_data.close()\n",
    "        print(\"save suceeded\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"total_feature save failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code_graph = get_graph(r'D:\\Andriod_GCN_andVGAEMalGAN\\data\\processed\\code_block_id_tr_reduced.npz')\n",
    "total_feature=get_overall_graph_feature(Code_graph)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "renewable-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_total_feature(r'code_total_feature',total_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        total_feature = pickle.load(f)\n",
    "        return total_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_api = load_feature(r'D:\\Andriod_GCN_andVGAEMalGAN\\training_api')\n",
    "api_dictionary = load_feature(r'D:\\Andriod_GCN_andVGAEMalGAN\\api_dictionary')\n",
    "reduce_api= load_feature(r'D:\\Andriod_GCN_andVGAEMalGAN\\reduce_api')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressed-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_api=np.array(reduce_api)\n",
    "reduce_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-socket",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YooChooseDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(YooChooseDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return []\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [r'code_block.dataset']\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "        \n",
    "    def process(self):\n",
    "#         pass\n",
    "                \n",
    "        data_list = []\n",
    "        edge_index_path=[r\"D:\\Andriod_GCN_andVGAEMalGAN\\data\\interim\\class0\",r\"D:\\Andriod_GCN_andVGAEMalGAN\\data\\interim\\class1\"]\n",
    "        count =0\n",
    "        current=0\n",
    "        for i in edge_index_path: \n",
    "            edge_indexes = glob(f'{i}/*B.npy')\n",
    "            for edge_index_path in edge_indexes:\n",
    "                outfile = os.path.basename(edge_index_path)\n",
    "                outfile=outfile.replace(\".B.npy\",\".npz\")\n",
    "#                 print(outfile)\n",
    "                current=current+1\n",
    "                print(current)\n",
    "                if count == 0:\n",
    "                    y=[0]\n",
    "                    save_path=r\"D:\\Andriod_GCN_andVGAEMalGAN\\data\\interim\\class0\\\\\"\n",
    "                else:\n",
    "                    y=[1]\n",
    "                    save_path=r\"D:\\Andriod_GCN_andVGAEMalGAN\\data\\interim\\class1\\\\\"\n",
    "                y = torch.tensor(y)\n",
    "                print(y)\n",
    "                edge_index = np.load(f'{edge_index_path}')\n",
    "                mask_B = np.nonzero(np.isin(edge_index, training_api).sum(axis=0) == 2)[0]\n",
    "                edge_index = edge_index[:, mask_B]\n",
    "                #edge_index = replace_with_dict(edge_index, api_dictionary)\n",
    "                k = np.array(list(api_dictionary.keys()))\n",
    "                #value is the new api\n",
    "                v = np.array(list(api_dictionary.values()))\n",
    "                # Get argsort indices\n",
    "                sidx = k.argsort()\n",
    "                ks = k[sidx]\n",
    "                vs = v[sidx]\n",
    "                edge_index_processed=vs[np.searchsorted(ks, edge_index)]\n",
    "                edge_index_processed = np.hstack([edge_index_processed, edge_index_processed[::-1, :]])\n",
    "                arr = np.unique(edge_index_processed, axis=1)\n",
    "                values = np.full(shape=edge_index_processed.shape[1], fill_value=1, dtype='i1')\n",
    "#                 try:\n",
    "                sparse_arr = sparse.coo_matrix(\n",
    "                    (values, (edge_index_processed[0], edge_index_processed[1])), shape=(len(training_api),len(training_api))\n",
    "                ).tocsc()\n",
    "\n",
    "                edge_index_processed = sparse_arr[reduce_api, :][:, reduce_api] \n",
    "                edge_index_processed=edge_index_processed.tocoo()\n",
    "                edge_index_processed.setdiag(0)\n",
    "                edge_index_processed.eliminate_zeros()\n",
    "                sparse.save_npz(os.path.join(save_path+outfile), edge_index_processed)\n",
    "                edge_index = torch.tensor([edge_index_processed.row.tolist(),edge_index_processed.col.tolist()], dtype=torch.long)\n",
    "                x = torch.tensor(total_feature,dtype=torch.float)\n",
    "                data = Data(x=x, edge_index=edge_index, y=y)\n",
    "                data_list.append(data)\n",
    "#                 except Exception as e:\n",
    "#                     print(e)\n",
    "#                     print(edge_index_path)\n",
    "            count=count+1\n",
    "                   \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset= YooChooseDataset(root=r'D:\\Andriod_GCN_andVGAEMalGAN\\dataset_graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-rabbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dataset: {custom_dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(custom_dataset)}')\n",
    "print(f'Number of features: {custom_dataset.num_features}')\n",
    "print(f'Number of classes: {custom_dataset.num_classes}')\n",
    "\n",
    "data = custom_dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
    "print(f'Contains self-loops: {data.contains_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "dataset = custom_dataset.shuffle()\n",
    "\n",
    "train_dataset = dataset[:3]\n",
    "test_dataset = dataset[3:]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=62, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=62, shuffle=False)\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(5, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, 2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        emb =x \n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x,emb\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(hidden_channels=32)\n",
    "# model.load_state_dict(torch.load(r\"C:\\Users\\Yumlum\\Desktop\\Experiment Result\\GCNModellocalglobal.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enable only when testing\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enable only during testing\n",
    "def get_embedding(data):\n",
    "    model.eval()\n",
    "    data=data.to(device)\n",
    "    out,emb = model(data.x, data.edge_index, data.batch)  \n",
    "    pred = out.argmax(dim=1)\n",
    "#     pred_numpy = pred.cpu().numpy()\n",
    "#     print(pred)\n",
    "    return pred,emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         data.x = data.x.to(device)\n",
    "         data.edge_index= data.edge_index.to(device)\n",
    "         data.batch=data.batch.to(device)\n",
    "         data.y = data.y.to(device)\n",
    "         out,emb = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad() # Clear gradients.\n",
    "         running_loss += loss.item() * data.num_graphs\n",
    "    running_loss /= len(train_loader.dataset)\n",
    "    return running_loss\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "     correct = 0\n",
    "     running_loss = 0.0\n",
    "     prediction=[]\n",
    "     truelabel=[]\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         data=data.to(device)\n",
    "         out,emb = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         pred_numpy = pred.cpu().numpy()\n",
    "         true_label = data.y.cpu().numpy()\n",
    "         loss = criterion(out, data.y)\n",
    "         running_loss += loss.item() * data.num_graphs\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "         prediction.append(pred_numpy)\n",
    "         truelabel.append(true_label)\n",
    "     running_loss /= len(train_loader.dataset)\n",
    "     return correct / len(loader.dataset),running_loss,prediction,truelabel  # Derive ratio of correct predictions.\n",
    "trainlosses = []\n",
    "testlosses=[]\n",
    "prediction=[]\n",
    "truelabel=[]\n",
    "for epoch in range(1, 300):\n",
    "    loss=train()\n",
    "    train_acc,train_loss,train_pred_numpy,train_true_label = test(train_loader)\n",
    "    trainlosses.append(train_loss)\n",
    "    test_acc,test_loss,pred_numpy,true_label = test(test_loader)\n",
    "    testlosses.append(test_loss)\n",
    "    prediction.append(pred_numpy)\n",
    "    truelabel.append(true_label)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f},Train loss:{train_loss:.4f},Test loss:{test_loss:.4f}')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enable only during testing\n",
    "edge_index_path=[r\"D:\\Andriod_GCN_andVGAEMalGAN\\data\\interim\\class0\",r\"D:\\Andriod_GCN_andVGAEMalGAN\\data\\interim\\class1\"]\n",
    "with open(r'embedding.csv', 'w') as f:\n",
    "    write = csv.writer(f) \n",
    "    count=0\n",
    "    index=0\n",
    "    prediction=[]\n",
    "    truelabel=[]\n",
    "    for i in edge_index_path:      \n",
    "        if count==0:\n",
    "            Type=0\n",
    "        if count==1:\n",
    "            Type=1\n",
    "        edge_indexes = glob(f'{i}/*.npz') \n",
    "        for j in edge_indexes:\n",
    "            try:\n",
    "                base_name=os.path.basename(j).replace(\".npz\",\"\")\n",
    "                edge_index = sparse.load_npz(j)\n",
    "                edge_index = torch.tensor([edge_index.row.tolist(),edge_index.col.tolist()], dtype=torch.long)  \n",
    "    #                 features=[]\n",
    "    #                 for i,j in zip(code_block_total_feature,local_centrality):\n",
    "    #                     features.append(i+j)\n",
    "                x = torch.tensor(total_feature,dtype=torch.float)\n",
    "                data = Data(x=x, edge_index=edge_index)\n",
    "                data.batch=torch.zeros(8781, dtype=torch.long)\n",
    "                pred,embedding=get_embedding(data)\n",
    "                pred_numpy = pred.cpu().numpy()\n",
    "                embs=embedding.cpu()\n",
    "                embedding=embs.detach().numpy().tolist()\n",
    "                prediction.append(pred_numpy)\n",
    "                truelabel.append(Type)\n",
    "                write.writerow([base_name]+embedding[0]+[Type])\n",
    "                index=index+1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"failed\")\n",
    "                print(j)\n",
    "        count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-camping",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionnew_test=[l.tolist() for l in prediction]\n",
    "predictionlist_test = [item for sublist in predictionnew_test for item in sublist]\n",
    "print(predictionlist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-seafood",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-project",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall\",recall_score(truelabel, predictionlist_test))\n",
    "print(\"precision\",precision_score(truelabel, predictionlist_test))\n",
    "print(\"f1 score\",f1_score(truelabel, predictionlist_test))\n",
    "print(\"accuracy score\",accuracy_score(truelabel, predictionlist_test))\n",
    "confusion_matrix(truelabel, predictionlist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure()\n",
    "# plt.plot(epochs, trainlosses, 'g', label='Training loss')\n",
    "# plt.plot(epochs, testlosses, 'b', label='validation loss')\n",
    "# plt.title('Training and Validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GraphConv\n",
    "\n",
    "\n",
    "class GraphSage(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GraphSage, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 =  SAGEConv(dataset.num_node_features, hidden_channels)\n",
    "        self.conv2 =  SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 =  SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        emb =x \n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x,emb\n",
    "\n",
    "model = GraphSage(hidden_channels=32)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-despite",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphSage(hidden_channels=32)\n",
    "print(model)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         data.x = data.x.to(device)\n",
    "         data.edge_index= data.edge_index.to(device)\n",
    "         data.batch=data.batch.to(device)\n",
    "         data.y = data.y.to(device)\n",
    "         out,emb = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad() # Clear gradients.\n",
    "         running_loss += loss.item() * data.num_graphs\n",
    "    running_loss /= len(train_loader.dataset)\n",
    "    return running_loss\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "     correct = 0\n",
    "     running_loss = 0.0\n",
    "     prediction=[]\n",
    "     truelabel=[]\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         data=data.to(device)\n",
    "         out,emb = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         pred_numpy = pred.cpu().numpy()\n",
    "         true_label = data.y.cpu().numpy()\n",
    "         loss = criterion(out, data.y)\n",
    "         running_loss += loss.item() * data.num_graphs\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "         prediction.append(pred_numpy)\n",
    "         truelabel.append(true_label)\n",
    "     running_loss /= len(train_loader.dataset)\n",
    "     return correct / len(loader.dataset),running_loss,prediction,truelabel  # Derive ratio of correct predictions.\n",
    "\n",
    "for epoch in range(1, 200):\n",
    "    loss=train()\n",
    "    train_acc,train_loss,train_pred_numpy,train_true_label = test(train_loader)\n",
    "    trainlosses.append(train_loss)\n",
    "    test_acc,test_loss,pred_numpy,true_label = test(test_loader)\n",
    "    testlosses.append(test_loss)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f},Train loss:{train_loss:.4f},Test loss:{test_loss:.4f}')\n",
    "#     if train_acc>=0.93 and test_acc >=0.93 :\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enable only during testing\n",
    "edge_index_path=[r\"D:\\Andriod_GCN_andVGAEMalGAN\\data\\interim\\class0\",r\"D:\\Andriod_GCN_andVGAEMalGAN\\data\\interim\\class1\"]\n",
    "with open(r'embeddingsage.csv', 'w') as f:\n",
    "    write = csv.writer(f) \n",
    "    count=0\n",
    "    index=0\n",
    "    prediction=[]\n",
    "    truelabel=[]\n",
    "    for i in edge_index_path:      \n",
    "        if count==0:\n",
    "            Type=0\n",
    "        if count==1:\n",
    "            Type=1\n",
    "        edge_indexes = glob(f'{i}/*.npz') \n",
    "        for j in edge_indexes:\n",
    "            try:\n",
    "                base_name=os.path.basename(j).replace(\".npz\",\"\")\n",
    "                edge_index = sparse.load_npz(j)\n",
    "                edge_index = torch.tensor([edge_index.row.tolist(),edge_index.col.tolist()], dtype=torch.long)  \n",
    "    #                 features=[]\n",
    "    #                 for i,j in zip(code_block_total_feature,local_centrality):\n",
    "    #                     features.append(i+j)\n",
    "                x = torch.tensor(total_feature,dtype=torch.float)\n",
    "                data = Data(x=x, edge_index=edge_index)\n",
    "                data.batch=torch.zeros(8781, dtype=torch.long)\n",
    "                pred,embedding=get_embedding(data)\n",
    "                pred_numpy = pred.cpu().numpy()\n",
    "                embs=embedding.cpu()\n",
    "                embedding=embs.detach().numpy().tolist()\n",
    "                prediction.append(pred_numpy)\n",
    "                truelabel.append(Type)\n",
    "                write.writerow([base_name]+embedding[0]+[Type])\n",
    "                index=index+1\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"failed\")\n",
    "                print(j)\n",
    "        count=count+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
